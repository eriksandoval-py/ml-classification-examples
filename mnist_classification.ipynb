{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74550463-5b6b-4df3-ba06-c041ebb59a2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classification\n",
    "Some of the most common supervised learnings tasks are regression and classification. Classification is a type of supervised learning task that predicts discrete classes. There are two types of classification as well, binary and multiclass. An example of binary would be 'spam email or not spam email'. Multiclass would be attempting to classify multiple types of animals, plants, etc.  Regression is a type of learning task that attempts to predict continuous numerical values, such as the temperature of a flame, or the height of an individual. In this notebook we will focus on classification. We will be using the MNIST dataset. It is a set of sabout 70,000 images of handwritten digits. Each image contains a corresponding label representing the actual digit.\n",
    "\n",
    "Thankfully, Scikit-Learn and Tensorflow contains a lot of functions that can be used to download datasets, with the MNIST dataset being one of them. You can download it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82c1b875-2ea9-4c5f-835b-5753ea36bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec60ac94-25d3-4f14-8813-778eb26ed2c0",
   "metadata": {},
   "source": [
    "The above code block handles all necessary imports to load in the data. It also prepares a training and test set as well by unpacking the data into tuples. Next we will have to normalize the data so that every value is between the values 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15d1232-cc04-4000-b958-f5b353f890eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fc98c-1f56-4a2a-8177-88215b95d118",
   "metadata": {},
   "source": [
    "Now we can begin to work on our model. We will use a basic sequential neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "266fd634-c17f-423d-9c5e-b154fef3cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a1a7f-4b1a-4986-999f-a951ff02d9c5",
   "metadata": {},
   "source": [
    "This creates the model, but we still have to add all the layers into it. We will use a Flatten layer, which basically takes the 28x28 grid and flattens it into an array of 786 pixels, each with a value that has been normalized to be between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95117f35-f290-4ff1-9384-4c63d75ad73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04936209-3656-4bda-9172-9c5288941f00",
   "metadata": {},
   "source": [
    "Once we have our first layer we will have to introduce a Dense layer. This represents one of our 'hidden' layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c86dc336-3b0b-4385-8d0c-2669d31e7735",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f272e3-be4b-44b6-8ade-c9f5e0b42fb9",
   "metadata": {},
   "source": [
    "Notice that the above code still utilizes an activation function called 'relu', short for rectified linear unit. There are many activation functions, such as the sigmoid activation function, which squashes the output to between 0 and 1. What relu does is create a threshold, and if the output value does not cross the threshold it will ouput 0, but if it does cross the threshold, it will ouput the value as it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac1fdc-c6ea-4ce5-90ab-fdf4238d42f1",
   "metadata": {},
   "source": [
    "Lets add another Dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3213aa4c-58e4-4a37-931d-7210fc068773",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea5551-8c51-487b-a028-f8bfc90f520b",
   "metadata": {},
   "source": [
    "Now we will add our output layer, which will be of size 10. Of course, they will represent the decimal system of numbers, from 0-9. The final layer will also be a Dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a0e588-aea0-48f0-b944-f681ef64b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8e85c-edb1-4667-a83e-cb877c6a8d84",
   "metadata": {},
   "source": [
    "As you can probably tell we have implemented the softmax activation function for our final layer. What this does is make the final 10 values sum up to 1. This is essentially a way of showing confidence in the prediction. For example, if we say that the first value has an ouput of 0.01, then that would indicate that the neural network does not think the y should be 0. However, if that first value were something like 0.95, then that would indicate that the neural network has high confidence that the digit is the number 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20873fbc-89d3-44f6-9728-c667cd83092f",
   "metadata": {},
   "source": [
    "Now we can compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb4eef18-07f6-4015-a7c6-b1c88e2cd9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d938e13-7a34-4e5f-87e5-fc0a070150b4",
   "metadata": {},
   "source": [
    "After that, we can fit the model to the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f869feb3-636e-4157-b0d6-71c814be7e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 19s 9ms/step - loss: 0.2598 - accuracy: 0.9239\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1055 - accuracy: 0.9675\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0719 - accuracy: 0.9771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d912448310>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa43a24-1a9b-42f0-8a37-9da62b93be2c",
   "metadata": {},
   "source": [
    "The epochs value we provided is essentially how many times we want our model to look at the training data. We can also save the model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf5a52b8-fd4b-4805-9a68-1f39435255c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mnist_classification.model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('mnist_classification.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5182a0-f5ec-43bb-896e-94dc54dd1fb3",
   "metadata": {},
   "source": [
    "Now let's try loading in the model for practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2c7305a-1fb4-484f-8740-366a64c83972",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('mnist_classification.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd153be-04e3-4b8d-99c8-51cc8f99e587",
   "metadata": {},
   "source": [
    "Once your model is trained on the training data, you will want to evaluate it on the test data. We can also unpack the loss and accuracy results of the evaluation using python's unpacking feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c50a5796-773c-4c78-b053-17951cd0cf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.1003 - accuracy: 0.9704\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d47034e-25dd-4097-8700-f2a4778e122f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10032467544078827, 0.9703999757766724)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ff111-9465-4d67-a00a-043834900645",
   "metadata": {},
   "source": [
    "What we want in the evaluation is a low loss and a high accuracy. Now we can try to test our model on our own handwritten digits. We can do this in paint or you can also scan them in and scale them down. Paint will be easier since you can manually edit the the image to be 28x28 pixels, just like our dataset. You will then predict using:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c648df9-b857-46fd-832f-48298cdb077a",
   "metadata": {},
   "source": [
    "`prediction = model.predict(image)`\n",
    "\n",
    "`print(f\"This digit is most likely: {np.argmax(prediction)}`\n",
    "\n",
    "`plt.imshow(image[0], cmap=plt.cm.binary)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
